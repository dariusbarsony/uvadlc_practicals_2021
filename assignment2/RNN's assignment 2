################
RNN's
################
?h(t) depends on all previous states of the hidden layer
: gradient dl/dw_ph = dl/d_y_hat * dy_hat/dp_t * dp_t/dW_ph
: gradient dl/dw_hh = dl/d_y_hat * dy_hat/dp_t * dp_t/dW_ph *\sumt_t'=0 (\productt_j=t'=1 dh_j/dh_j-1)*dh_t'/dW_hh
:dW_hx, expressed accordingly

?conceptually
!question/video

q2.1 (4 points):
- a; expand dl_dW_hh (for t=3),:"to get myself familiar with what the equation entails"
- b; what problems occur when training the network for a large number of time steps

#############
LSTM's
#############
definition: 

problem: influence of an input on a hidden layer: decays or blows up
- vanishing gradients = main problem of vanilla rnn's
- lstm's have been introduced to overcome this problem

step by step: 
- cell state: horizontal line running through the top of the diagram
- gates: control information throughflow

sigmoid gate layer:
- outputs number between 0 and 1: determines whether information should be completely kept or completely discarded. 
- h_t-1 and x_t 

decide what information needs to be stored in the cell state
- input gate layer
- sigmoid layer; input gate layer: decides which values we'll update
- tanh creates a vector of new candidate values that could be added to the state

combined to create an update to the state

- add 1 to forget gate bias
- forward pass
- wat moet sample doen


################
math
################

q2.2 (8 points)
- write down purpose of the gates
- total number of trainable parameters as defined by the formula defined above

#################
LSTM's in Pytorch
#################

q2.3 (7 points):
- implement lstm as specified by equations 8-13

! no high level pytorch funcs: 
: not allowed: nn.Linear, nn.LSTM, nn.LSTMCell
: allowed; nn.parameter, torch.sigmoid

# assignment description
- use embeddings instead of one hot representation of characters
- we train to learn the embedding

weight initialization: 
uniformly between: -1 / np.sqrt(N_hidden) and 1 / np.sqrt(N_hidden)
- add a bias of 1 to the forget gate --> makes it so that the model starts off by remembering old states and learns what to forget

- linear output mapping: does not need to be learned, (the computation of p(t)) 
- no need for backward, rely on automatic computation
- variable sequence length and batch size

q2.4:
! use embeddings to represent each character of the input sequence

q2.4a
q2.4b
q2.4c


